"""
Unified LLM text generation with Groq primary and Gemini fallback
Enhanced with automatic provider failover
"""

import os
import time
import logging

from src.core.llm_client import UnifiedLLMClient
from src.utils.exceptions import APIError

logger = logging.getLogger(__name__)

# Singleton unified client
_llm_client: 'UnifiedLLMClient | None' = None


def _get_client() -> UnifiedLLMClient:
    """Get or create singleton unified LLM client"""
    global _llm_client
    if _llm_client is None:
        _llm_client = UnifiedLLMClient()
    return _llm_client


def generate_text(system_prompt: str, user_prompt: str, max_retries: int = 3) -> str:
    """
    Generate text using Groq (primary) with Gemini fallback
    
    The UnifiedLLMClient handles automatic failover:
    1. Try Groq first (fast, high quota)
    2. On any Groq error → Gemini
    3. On Gemini error → raise exception
    
    Args:
        system_prompt: System instruction/context
        user_prompt: User's actual prompt
        max_retries: Not used (UnifiedLLMClient handles retries internally)
        
    Returns:
        Generated text response (trimmed)
        
    Raises:
        APIError: If both Groq and Gemini fail
        Exception: If unexpected error occurs
    """
    client = _get_client()
    
    # Combine prompts (LLMs don't support separate system role in all APIs)
    combined_prompt = f"{system_prompt}\n\n{user_prompt}"
    
    try:
        text, provider = client.generate_content(combined_prompt)
        logger.debug(f"Text generated by {provider.upper()} ({len(text)} chars)")
        return text
    except Exception as e:
        logger.error(f"Text generation failed: {e}")
        raise APIError(f"Failed to generate text: {e}")
